{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description:\n",
    "这个笔记本主要是先把YoutubeDNN召回跑起来， 采用的数据集是data_process下面的train_data.csv， 这个是用户历史行为数据+用户画像数据， 模型的话先调用包实现， 这里面主要完成的步骤是：\n",
    "1. 导入数据， 划分出测试集来， 最后一天用户id进行评估\n",
    "2. 用训练集的数据构造YoutubeDNN的数据集出来， 这里采用滑窗的方式构造， 但由于有些序列很长，尝试通过滑动步长优化\n",
    "3. 构造YoutubeDNN的输入， 由于是调用deepmatch的YoutubeDNN，所以需要把输入特殊构造\n",
    "4. 建立YoutubeDNN模型训练\n",
    "5. 从YoutubeDNN中拿到用户embedding和doc的embedding，保存起来\n",
    "6. 用第11天的测试集，进行评估，看看YoutubeDNN的召回效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'deepmatch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4a4252608582>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepmatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msampledsoftmaxloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepmatch'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import random\n",
    "from datetime import datetime\n",
    "import collections\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from deepmatch.models import *\n",
    "from deepmatch.utils import sampledsoftmaxloss\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# 从工具包里导入工具函数\n",
    "from utils import gen_data_set, gen_model_input, train_youtube_model\n",
    "from utils import get_embeddings, get_youtube_recall_res\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"  # 1显示所有信息， 2显示error和warnings, 3显示error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.导入数据，划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"data_process\"\n",
    "data = pd.read_csv(os.path.join(data_path,\"train_data.csv\"), index_col=0,parse_dates=[\"expo_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_time = data['expo_time'].max()\n",
    "\n",
    "# 添加example_age字段\n",
    "# 沿用 example_age的定义 从最近时间 - 曝光的时间\n",
    "data[\"example_age\"] = (pd.to_datetime(latest_time) - data['expo_time'])\n",
    "# 转成小时的形式 上面两式子相减是pandas的timedelta类型， 只有days和seconds属性\n",
    "data[\"example_age\"] = data[\"example_age\"].apply(lambda x : x.days*24 + x.seconds // 3600)\n",
    "\n",
    "minmax = MinMaxScaler() # 归一化\n",
    "data[\"example_age\"] = minmax.fit_transform(data[\"example_age\"].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择出需要用到的列\n",
    "use_cols = ['user_id', 'article_id','expo_time','net_status','exop_position','duration','device','city','age','gender','example_age','click']\n",
    "data_new = data[use_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_new['exop_position'].unique().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 划分测试集和训练集\n",
    "* 训练集， 每个用户的历史点击，去掉最后一次\n",
    "* 测试集， 每个用户的最后一次点击"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按照用户分组，然后把最后一个item拿出来\n",
    "click_df = data_new[data_new['click']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "click_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hist_and_last_click(all_click):\n",
    "    all_click = all_click.sort_values(by=['user_id', 'expo_time'])\n",
    "    click_last_df = all_click.groupby('user_id').tail(1)\n",
    "    \n",
    "    # 如果用户只有一个点击，hist为空了，会导致训练的时候这个用户不可见，此时默认泄露一下\n",
    "    def hist_func(user_df):\n",
    "        if len(user_df) == 1:\n",
    "            return user_df\n",
    "        else:\n",
    "            return user_df[:-1]\n",
    "\n",
    "    click_hist_df = all_click.groupby('user_id').apply(hist_func).reset_index(drop=True)\n",
    "\n",
    "    return click_hist_df, click_last_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_click_hist_df, user_click_last_df = get_hist_and_last_click(click_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. YoutubeDNN召回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def youtubednn_recall(data, topk=200, embedding_dim=8, his_seq_maxlen=50, negsample=0,\n",
    "                      batch_size=64, epochs=1, verbose=1, validation_split=0.0):\n",
    "    \"\"\"通过YouTubeDNN模型，计算用户向量和文章向量\n",
    "    param: data: 用户日志数据\n",
    "    topk: 对于每个用户，召回多少篇文章\n",
    "    \"\"\"\n",
    "    user_id_raw = data[['user_id']].drop_duplicates('user_id')\n",
    "    doc_id_raw = data[['article_id']].drop_duplicates('article_id')\n",
    "    \n",
    "    # 类别数据编码   \n",
    "    base_features = ['user_id', 'article_id', 'city', 'age', 'gender']\n",
    "    feature_max_idx = {}\n",
    "    for f in base_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[f] = lbe.fit_transform(data[f])\n",
    "        feature_max_idx[f] = data[f].max() + 1\n",
    "        \n",
    "    # 构建用户id词典和doc的id词典，方便从用户idx找到原始的id\n",
    "    user_id_enc = data[['user_id']].drop_duplicates('user_id')\n",
    "    doc_id_enc = data[['article_id']].drop_duplicates('article_id')\n",
    "    user_idx_2_rawid = dict(zip(user_id_enc['user_id'], user_id_raw['user_id']))\n",
    "    doc_idx_2_rawid = dict(zip(doc_id_enc['article_id'], doc_id_raw['article_id']))\n",
    "    \n",
    "    # 保存下每篇文章的被点击数量， 方便后面高热文章的打压\n",
    "    doc_clicked_count_df = data.groupby('article_id')['click'].apply(lambda x: x.count()).reset_index()\n",
    "    doc_clicked_count_dict = dict(zip(doc_clicked_count_df['article_id'], doc_clicked_count_df['click']))\n",
    "\n",
    "    train_set, test_set = gen_data_set(data, doc_clicked_count_dict, negsample, control_users=False)\n",
    "    \n",
    "    # 构造youtubeDNN模型的输入\n",
    "    train_model_input, train_label = gen_model_input(train_set, his_seq_maxlen)\n",
    "    test_model_input, test_label = gen_model_input(test_set, his_seq_maxlen)\n",
    "    \n",
    "    # 构建模型并完成训练\n",
    "    model = train_youtube_model(train_model_input, train_label, embedding_dim, feature_max_idx, his_seq_maxlen, batch_size, epochs, verbose, validation_split)\n",
    "    \n",
    "    # 获得用户embedding和doc的embedding， 并进行保存\n",
    "    user_embs, doc_embs = get_embeddings(model, test_model_input, user_idx_2_rawid, doc_idx_2_rawid)\n",
    "    \n",
    "    # 对每个用户，拿到召回结果并返回回来\n",
    "    user_recall_doc_dict = get_youtube_recall_res(user_embs, doc_embs, user_idx_2_rawid, doc_idx_2_rawid, topk)\n",
    "    \n",
    "    return user_recall_doc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_recall_doc_dict = youtubednn_recall(user_click_hist_df, negsample=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "05ece30799c2dcdac4c13b3af20453da19de8df0d9a1de52cff7e0b6e1e82bdd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
